{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> One Layer Neural Network </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2> Functions for pushing numbers forward </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    " \n",
    "# give it the weight and inputs and it will calculate the results\n",
    "def feedInputsForward(inputs,weights):\n",
    "    outputArr = []\n",
    "    \n",
    "    # if there are multiple hidden layers, you need to have a set of weights for each input to HL connection\n",
    "    if(weights.ndim > 1):\n",
    "        for each in weights:\n",
    "            outputArr.append(gradiantDescentFormula(np.dot(each,inputs)))\n",
    "        return np.array(outputArr)\n",
    "    \n",
    "    # if there is only one output/hidden layer, just do the dot product of the 1-d arrays\n",
    "    else:\n",
    "        outputArr.append(gradiantDescentFormula(np.dot(inputs,weights)))\n",
    "        return np.array(outputArr)\n",
    "\n",
    "\n",
    "# gradiant descent is defined as 1 / (1 + e^(-x))\n",
    "# this formula does that though imperfectly\n",
    "def gradiantDescentFormula(x):\n",
    "    return (1/(1 + (math.e ** (-x)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Backpropagation Error Calculation Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# calculates the total error in the network i.e. difference betweeen target and output\n",
    "def totalError(target, y):\n",
    "    return (1/2) * ((target - y) ** 2)\n",
    "\n",
    "# calculates error for the output unit y\n",
    "def errorOfY(target,y):\n",
    "    return y * (1 - y) * (target - y)\n",
    "\n",
    "# calculates the error for each hidden node\n",
    "# PARAM: hl is the value calculated for that hidden layer\n",
    "# PARAM: weight is the weight of that specified connection\n",
    "# PARAM: error of Y is the error of the output unit y\n",
    "def hiddenNodeError(hl,weight, errorOfY):\n",
    "    return hl * (1 - hl) * (weight * errorOfY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Learning Functions </h2>\n",
    "This one is more complicated so I will be doing a lot of documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that calculates the new weight for the HL to output\n",
    "# PARAM: currentWeight is the weight of the connection between the hidden node and the output\n",
    "# PARAM: learningFactor is an overall value for the network\n",
    "# PARAM: errorOfY is the error for the output unit y\n",
    "# PARAM: hlValue is the value at the HL\n",
    "def updateWeightOfHLToY(currentWeight,learningFactor,errorOfY,hlValue):\n",
    "    return currentWeight + (learningFactor * errorOfY * hlValue)\n",
    "\n",
    "# this is the function that calculates the new weight for the input to HL\n",
    "# PARAM: currentWeight is the weight of the connection between the input and the hidden node\n",
    "# PARAM: learningFactor is an overall value for the network\n",
    "# PARAM: errorOfHL is the error of the hidden node\n",
    "# PARAM: inputVal is the value of input\n",
    "def updateWeightofInputToHL(currentWeight,learningFactor,errorOfHL,inputVal):\n",
    "    return currentWeight + (learningFactor * errorOfHL * inputVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Architecture Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6373836 , 0.97772114, 0.48604013])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# generates num of hidden nodes\n",
    "def howManyHN(numInputs, numOutputs):\n",
    "    HN = (2/3) * (numInputs + numOutputs)\n",
    "    return int(round(HN,0))\n",
    "\n",
    "# generates weights for inputs to HL\n",
    "def generateWeightsForInputToHL(numInputs,numH):\n",
    "    inputToHl = []\n",
    "    for i in range(numH):\n",
    "        inputToHl.append(np.random.uniform(0,1,numInputs + 1))\n",
    "    return np.array(inputToHl)\n",
    "\n",
    "# generates weights for hl to out\n",
    "# asssumes there is only 1 output\n",
    "def generateWeightsFromHlToOut(numOutputs, numH):\n",
    "    HlToOut = np.random.uniform(0,1,numH + 1)\n",
    "    \n",
    "    #for i in range(numOutputs):\n",
    "       # HlToOut.append(np.random.uniform(0,1,numH + 1))\n",
    "    \n",
    "    return np.array(HlToOut)\n",
    "generateWeightsFromHlToOut(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> One Epoch </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this assumes the input array is a 2D array with the format [[x,n],[x,n] ... , [x,n]]\n",
    "\n",
    "def oneEpoch(inputArray,numOutputs,target, bias,learningFactor):\n",
    "    \n",
    "    # calculate how many hidden nodes\n",
    "    numHN = howManyHN(len(inputArray[0]),numOutputs)\n",
    "    \n",
    "    # TODO: Generate Random Weights; for now I am hard coding weights I know the results of\n",
    "    #       to verify it is working\n",
    "    \n",
    "    # Hardcoded Inital Input to HL Weights\n",
    "    # inputToHLW = [[1,1,0.5],[1,-1,2]]\n",
    "    # numpyinputToHLW = np.array(inputToHLW)\n",
    "    \n",
    "    # Hard coded Inital HL to Out Weights \n",
    "    # HLToOutW = [1,1.5,-1]\n",
    "    # numpyHlToOutW = np.array(HLToOutW)\n",
    "    \n",
    "    # Intial random weights for input to hl and hl to output\n",
    "    numpyinputToHLW = generateWeightsForInputToHL(len(inputArray[0]),numHN)\n",
    "    numpyHlToOutW = generateWeightsFromHlToOut(numOutputs,numHN)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if not isinstance(inputArray, np.ndarray):\n",
    "        numpyInput = np.array(inputArray)\n",
    "    else:\n",
    "        numpyInput = inputArray\n",
    "    \n",
    "    for i in range(len(numpyInput)):\n",
    "         \n",
    "        \n",
    "        # get the row of inputs from the value then append bias\n",
    "        numpyRow = numpyInput[i]\n",
    "        numpyRow = np.insert(numpyRow,0,bias)\n",
    "        # print(numpyRow)\n",
    "        # array for hidden node values\n",
    "        hiddenNodeValues = []\n",
    "    \n",
    "        # array for the results\n",
    "        results = []\n",
    "    \n",
    "        \n",
    "    \n",
    "        # get the hidden node values\n",
    "        hiddenNodeValues = feedInputsForward(numpyRow,numpyinputToHLW)\n",
    "    \n",
    "        # put bias back in\n",
    "        hiddenNodeValues = np.insert(hiddenNodeValues,0,bias)\n",
    "    \n",
    "        # calculate results\n",
    "        results = feedInputsForward(hiddenNodeValues,numpyHlToOutW)\n",
    "    \n",
    "        # print(hiddenNodeValues)\n",
    "        if(i % 250 == 0):\n",
    "            print(results)\n",
    "        \n",
    "        # print(type(hiddenNodeValues))\n",
    "        # print(type(results))\n",
    "    \n",
    "        # calculate total error and error on the output y if it is there\n",
    "        totalE = -1\n",
    "        ynumpyHlToOutW = -1\n",
    "    \n",
    "        # TODO: figure out how to do this with more than one output\n",
    "        if(len(results) == 1):\n",
    "            totalE = totalError(bias,results[0])\n",
    "            yError = errorOfY(target,results[0])\n",
    "    \n",
    "        # print(totalE)\n",
    "        # print(yError)\n",
    "    \n",
    "    \n",
    "        \n",
    "        # calculate the error for each hidden node value and store it in an array\n",
    "        # assumes that there is only one output\n",
    "        hiddenNodeErrorArr = []\n",
    "        i = 1\n",
    "        # print(hiddenNodeValues)\n",
    "        while (i < len(hiddenNodeValues)):\n",
    "            # print(\"test\")\n",
    "            hiddenNodeErrorArr.append(hiddenNodeError(hiddenNodeValues[i],numpyHlToOutW[i],yError))\n",
    "            i += 1\n",
    "        \n",
    "        # print(hiddenNodeErrorArr)\n",
    "        # print()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        # Update the weights of HL to Y\n",
    "        i = 0\n",
    "        for each in numpyHlToOutW:\n",
    "            # print(\"Before Weight HL to Y: %s\" % each)\n",
    "            each = updateWeightOfHLToY(each,learningFactor,yError,hiddenNodeValues[i])\n",
    "            i += 1\n",
    "            # print(\"After Weight HL t Y: %s\" % each)\n",
    "            # print()\n",
    "   \n",
    "    \n",
    "    \n",
    "        # Update the weights from input to HL\n",
    "        # this assumes there is more than one set of weights (i.e. more than one input and one hidden layer)\n",
    "        i = 0\n",
    "        for each in numpyinputToHLW:\n",
    "            j = 0\n",
    "            for num in each:\n",
    "                # print(\"Before Weight of input to HL: %s\" % numpyinputToHLW[i][j])\n",
    "                # print(\"Hidden Node Error: %s\" % hiddenNodeErrorArr[i])\n",
    "                # print(\"Input: %s\" % numpyInput[j])\n",
    "                numpyinputToHLW[i][j] = updateWeightofInputToHL(numpyinputToHLW[i][j],learningFactor,hiddenNodeErrorArr[i],numpyRow[j])\n",
    "                # print(\"After Weight of input to HL: %s\" % numpyinputToHLW[i][j])\n",
    "                # print()\n",
    "                j += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main </h3> \n",
    "This is what I used for testing while I was buidling the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1: 0.8175744761936437\n",
      "Hidden Layer 2: 0.9525741268224331\n",
      "y = 0.7813904309473313\n",
      "Total Error: 0.023895071840696763\n",
      "Error of Y: 0.037342760966238966\n",
      "HL1 Error: 0.008354310462937586\n",
      "HL2 Error: -0.001687021205584568\n",
      "Updated Weight for Bias to Y: 1.0152652441182985\n",
      "Updated Weight for input1 to h1 Weight: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# intital varaibles for back propigation later\n",
    "HL1 = 0\n",
    "HL2 = 0\n",
    "y = 0\n",
    "\n",
    "# all the starting weights for the input to hidden layer\n",
    "# start with bias then H1-1 to HL-n \n",
    "bias = 1\n",
    "HLW = [[1,1,0.5],[1,-1,2]]\n",
    "numpyHLW = np.array(HLW)\n",
    "\n",
    "# set the inputs and bias\n",
    "inputs = [bias,0,1]\n",
    "numpyInputs = np.array(inputs)\n",
    "\n",
    "\n",
    "# set the inputs for the hidden to output layer\n",
    "numpyHlToOut = feedInputsForward(numpyInputs,numpyHLW)\n",
    "\n",
    "# record the results\n",
    "HL1 = numpyHlToOut[0]\n",
    "HL2 = numpyHlToOut[1]\n",
    "\n",
    "# put bias back in at the front\n",
    "numpyHlToOut = np.insert(numpyHlToOut,0,bias)\n",
    "\n",
    "\n",
    "# all the weights for the hidden to output layer\n",
    "hlToOutW = [1,1.5,-1]\n",
    "numpyHlToOutW = np.array(hlToOutW)\n",
    "\n",
    "# get the results\n",
    "results = feedInputsForward(numpyHlToOut,numpyHlToOutW)\n",
    "\n",
    "# record the results\n",
    "y = results[0]\n",
    "\n",
    "\n",
    "# print the results\n",
    "print(\"Hidden Layer 1: %s\" % HL1)\n",
    "print(\"Hidden Layer 2: %s\" % HL2)\n",
    "print(\"y = %s\" % y)\n",
    "\n",
    "\n",
    "# start of backpropogation part\n",
    "\n",
    "totalE = totalError(1,y)\n",
    "print(\"Total Error: %s\" % totalE)\n",
    "\n",
    "yError = errorOfY(1,y)\n",
    "print(\"Error of Y: %s\" % yError)\n",
    "\n",
    "h1Error = hiddenNodeError(HL1,numpyHlToOutW[1],yError)\n",
    "h2Error = hiddenNodeError(HL2,numpyHlToOutW[2],yError)\n",
    "print(\"HL1 Error: %s\" % h1Error)\n",
    "print(\"HL2 Error: %s\" % h2Error)\n",
    "\n",
    "# calculating updated weights for HL to Y\n",
    "\n",
    "# bias\n",
    "newBiasToY = updateWeightOfHLToY(numpyHlToOutW[0],0.5,yError,HL1)\n",
    "print(\"Updated Weight for Bias to Y: %s\" % newBiasToY)\n",
    "\n",
    "# calulating error for input to HL\n",
    "\n",
    "# new input 1 to h1 weight \n",
    "newInput1toH1 = updateWeightofInputToHL(numpyHLW[0][1],0.5,h1Error,inputs[1])\n",
    "print(\"Updated Weight for input1 to h1 Weight: %s\" % newInput1toH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
